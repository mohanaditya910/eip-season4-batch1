{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "max-12k-LRS-L1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanaditya910/eip-season4-batch1/blob/master/week2/final_submission_max_12k_LRS_L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03uXvNxwKz5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "1e4ee7e9-503a-4d61-eefc-f6345f45d307"
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "#image standardization........\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#image normalization................\n",
        "X_train=(X_train-np.mean(X_train))/np.std(X_train)\n",
        "X_test=(X_test-np.mean(X_test))/np.std(X_test)\n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-e6V334K7sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#praying to the heavenly gods........................\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Activation,Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l1,l1_l2,l2\n",
        "from keras.initializers import TruncatedNormal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNJC5b7jK_kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def skeleton(input_shape,num_classes,n_c_factor_3,n_c_1,layers_in_block,dropout,l1_value):\n",
        "\n",
        "  model=Sequential()\n",
        "  for i in range(layers_in_block):\n",
        "    if i==0:\n",
        "      model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,input_shape=(28,28,1),activation='relu',kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "      model.add(BatchNormalization())\n",
        "    else:\n",
        "      model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,activation='relu',kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "      model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
        "  model.add(Conv2D(filters=n_c_1,kernel_size=1,activation='relu',kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  for i in range(layers_in_block):\n",
        "    model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,activation='relu',kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "    model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "  #no maxpooling\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=1,activation='relu',kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  #activation is avoided.\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=3,kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=3,kernel_regularizer=l1(l1_value),use_bias=False,kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  ##\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPIYHdKuZJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch<9:\n",
        "    return round(0.004 * 1/(1 + 0.319 * epoch), 10)\n",
        "  else:\n",
        "    return round(0.0004 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch<=6:\n",
        "    return round(0.004 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch in [7,8,9,11,12,13,15,16,17,19]:\n",
        "    return round(0.0004 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "  elif epoch in [10,14]:\n",
        "    return round(0.004 * 1/(1 + 0.319 * epoch), 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM3c94gRLDvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "outputId": "dd3788ce-c2a2-40eb-adff-14a9dba7e4ba"
      },
      "source": [
        "input_shape=(28,28,1)\n",
        "num_classes=10\n",
        "layers_in_block=3\n",
        "\n",
        "\n",
        "n_c_factor_3=8\n",
        "n_c_1=8\n",
        "dropout=0.05\n",
        "opt=Adam(lr=0.004)\n",
        "l1_value=0.00005\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch<=6:\n",
        "    return round(0.004 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch in [7,8,9,11,12,13,14,16,17,18,19]:\n",
        "    return round(0.0004 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "  elif epoch in [10,15]:\n",
        "    return round(0.004 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model=skeleton(input_shape,num_classes,n_c_factor_3,n_c_1,layers_in_block,dropout,l1_value)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#l1 0.00001 99.31\n",
        "#l1 0.0001  99.31, \n",
        "#l1 0.0001  99.31, do 0.1\n",
        "#lr0.002    99.27\n",
        "#lr0.004, l1 0.0001 99.32\n",
        "#lr0.004  l1 0.00001 99.17\n",
        "#lr0.004  l1 0.000001 99.29\n",
        "#truncated initializer\n",
        "\n",
        "#lr0.004  l1 0.00001  99.24\n",
        "#lr0.004  l1 0.00001 lrs 99.34 consistently hitting 99.3 overfitting.. to reduce (reducing lr further by 10)\n",
        "#lr0.004  l1 0.00005 lrs 99.38 same pattern...\n",
        "#lr0.004  l1 0.0001 lrs 99.33\n",
        "#lr0.004  l1 0.00005 new lrs(4,4 to find local min) 99.48"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_91 (Conv2D)           (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 24, 24, 16)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 22, 22, 24)        3456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 22, 22, 24)        96        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 22, 22, 24)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 11, 11, 8)         192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 11, 11, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 9, 9, 8)           576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 9, 9, 8)           32        \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 7, 7, 16)          1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 5, 5, 24)          3456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 5, 5, 10)          240       \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 3, 3, 10)          900       \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 1, 1, 10)          900       \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 12,512\n",
            "Trainable params: 12,304\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_qd80VJLHVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21d2c64c-d48b-44b1-a5f2-3f756973af44"
      },
      "source": [
        "history=model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1,validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.004.\n",
            "60000/60000 [==============================] - 45s 743us/step - loss: 0.2222 - acc: 0.9461 - val_loss: 0.1287 - val_acc: 0.9749\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0030326005.\n",
            "60000/60000 [==============================] - 38s 627us/step - loss: 0.1239 - acc: 0.9787 - val_loss: 0.1082 - val_acc: 0.9839\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0024420024.\n",
            "60000/60000 [==============================] - 38s 628us/step - loss: 0.1110 - acc: 0.9822 - val_loss: 0.1037 - val_acc: 0.9855\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0020439448.\n",
            "60000/60000 [==============================] - 38s 627us/step - loss: 0.1016 - acc: 0.9849 - val_loss: 0.1004 - val_acc: 0.9861\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0017574692.\n",
            "60000/60000 [==============================] - 38s 626us/step - loss: 0.0931 - acc: 0.9865 - val_loss: 0.0810 - val_acc: 0.9899\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0015414258.\n",
            "60000/60000 [==============================] - 38s 626us/step - loss: 0.0856 - acc: 0.9880 - val_loss: 0.0819 - val_acc: 0.9898\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0013726836.\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0806 - acc: 0.9893 - val_loss: 0.0852 - val_acc: 0.9886\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001237241.\n",
            "60000/60000 [==============================] - 37s 619us/step - loss: 0.0660 - acc: 0.9938 - val_loss: 0.0678 - val_acc: 0.9933\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001126126.\n",
            "60000/60000 [==============================] - 38s 626us/step - loss: 0.0604 - acc: 0.9952 - val_loss: 0.0652 - val_acc: 0.9935\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001033325.\n",
            "60000/60000 [==============================] - 38s 626us/step - loss: 0.0589 - acc: 0.9952 - val_loss: 0.0638 - val_acc: 0.9945\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0009546539.\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0717 - acc: 0.9914 - val_loss: 0.0699 - val_acc: 0.9912\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 8.87115e-05.\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0590 - acc: 0.9953 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 8.285e-05.\n",
            "60000/60000 [==============================] - 37s 619us/step - loss: 0.0555 - acc: 0.9959 - val_loss: 0.0620 - val_acc: 0.9940\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 7.77152e-05.\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0537 - acc: 0.9960 - val_loss: 0.0613 - val_acc: 0.9936\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 7.31797e-05.\n",
            "60000/60000 [==============================] - 37s 618us/step - loss: 0.0531 - acc: 0.9963 - val_loss: 0.0609 - val_acc: 0.9939\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0006914434.\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0620 - acc: 0.9932 - val_loss: 0.0643 - val_acc: 0.9925\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 6.55308e-05.\n",
            "60000/60000 [==============================] - 38s 627us/step - loss: 0.0537 - acc: 0.9957 - val_loss: 0.0603 - val_acc: 0.9948\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 6.22762e-05.\n",
            "60000/60000 [==============================] - 38s 627us/step - loss: 0.0506 - acc: 0.9967 - val_loss: 0.0599 - val_acc: 0.9942\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 5.93296e-05.\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0496 - acc: 0.9969 - val_loss: 0.0594 - val_acc: 0.9945\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 5.66492e-05.\n",
            "60000/60000 [==============================] - 37s 619us/step - loss: 0.0488 - acc: 0.9971 - val_loss: 0.0589 - val_acc: 0.9942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGw9NfvJLL5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8ee4df78-9b30-42db-eb1f-c03d2a33a0f8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,acc,'b',label='Train_acc')\n",
        "plt.plot(epochs,val_acc,'g',label='Val_acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxBzPduhLO3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f4c69caf-070a-4877-9dfd-52a24871fd0b"
      },
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 143us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05891113919615745, 0.9942]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_UkC9g97Z8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "As we are going to use regularizer, i thought of checking out with different Learning Rates and 0.04 gave me the best value. So i stuck with it...\n",
        "\n",
        "Next part is introducing truncated initializer..which according to the keras documentation performs better on weights and filters..generally nature follows bell curve distribution..so went for this...\n",
        "\n",
        "then started to play with l1 regularizer value...from previous experience i used 3 values for regularizers 0.00005,0.00001,0.0001(99.38,99.34,99.33 val_acc). 0.00005 is in between the other values and its obtained val_acc is greatest among the 3 l1 values , hence its kind of peaking. so better to stay with that.\n",
        "\n",
        "for all these values, val_acc,train_acc plots in (epochs and acc) the curves appear to be smooth for the last 7-8 epochs.\n",
        "\n",
        "now the redifining and better using Learning rate scheduler to explore the local minimas exposed by the regularizer.\n",
        "\n",
        "for the last 10 epochs,decrement of by 10 (lr*0.1) to the learning rate scheduler already provided is done.\n",
        "\n",
        "this leads to the exploration of only one minima which may or may not contain the required minima.\n",
        "\n",
        "hence to give the model a chance to better the exploration, i have taken 3 batches of epochs of size 4 ([7,8,9,10],[12,13,14,15],[17,18,19]) and then applied learning rate as obtained above. but for the left over values i have not decremnted by 10 meaning, the value as it is provided by the previous scheduler is taken. this will make the model to explore 3 neighborhood of minimas giving it a better chance to find the best weights\n",
        "\n",
        "this has given consistent values..of val_acc over 99.4 in epochs 11 to 20...\n",
        " \n",
        "this can be observed in logs..."
      ]
    }
  ]
}