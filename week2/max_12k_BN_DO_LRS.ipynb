{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "max-12k-BN-DO-LRS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanaditya910/eip-season4-batch1/blob/master/week2/max_12k_BN_DO_LRS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR_NTG7ExfpD",
        "colab_type": "code",
        "outputId": "2064e938-0db1-406d-94ee-67bfe1fc94d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "#image standardization........\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#image normalization................\n",
        "X_train=(X_train-np.mean(X_train))/np.std(X_train)\n",
        "X_test=(X_test-np.mean(X_test))/np.std(X_test)\n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEj200cdxm1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#praying to the heavenly gods........................\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Activation,Dropout\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShZNPr2dxoY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def skeleton(input_shape,num_classes,n_c_factor_3,n_c_1,layers_in_block,dropout):\n",
        "\n",
        "  model=Sequential()\n",
        "  for i in range(layers_in_block):\n",
        "    if i==0:\n",
        "      model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,input_shape=(28,28,1),activation='relu',use_bias=False))\n",
        "      model.add(BatchNormalization())\n",
        "    else:\n",
        "      model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,activation='relu',use_bias=False))\n",
        "      model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
        "  model.add(Conv2D(filters=n_c_1,kernel_size=1,activation='relu',use_bias=False))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  for i in range(layers_in_block):\n",
        "    model.add(Conv2D(((i+1)*n_c_factor_3),kernel_size=3,activation='relu',use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "  #no maxpooling\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=1,activation='relu',use_bias=False))\n",
        "  #activation is avoided.\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=3,use_bias=False))\n",
        "  model.add(Conv2D(filters=num_classes,kernel_size=3,use_bias=False))\n",
        "  ##\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a1RlALtxuyu",
        "colab_type": "code",
        "outputId": "94bad21a-0a6d-4824-bffe-d20df5f64f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_shape=(28,28,1)\n",
        "num_classes=10\n",
        "layers_in_block=3\n",
        "\n",
        "\n",
        "n_c_factor_3=8\n",
        "n_c_1=8\n",
        "dropout=0.05\n",
        "opt=Adam(lr=0.003)\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model=skeleton(input_shape,num_classes,n_c_factor_3,n_c_1,layers_in_block,dropout)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 24)        3456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 24)        96        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 22, 22, 24)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 8)         192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 8)           576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 8)           32        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 24)          3456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 10)          240       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 10)          900       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 1, 1, 10)          900       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 12,512\n",
            "Trainable params: 12,304\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FV9u9Abxwmq",
        "colab_type": "code",
        "outputId": "4151f365-cafe-47cf-9e9c-289c6d4b4b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1,validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 40s 669us/step - loss: 0.1653 - acc: 0.9484 - val_loss: 0.0650 - val_acc: 0.9794\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 35s 584us/step - loss: 0.0606 - acc: 0.9811 - val_loss: 0.0683 - val_acc: 0.9792\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 35s 587us/step - loss: 0.0477 - acc: 0.9852 - val_loss: 0.0300 - val_acc: 0.9901\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 36s 595us/step - loss: 0.0395 - acc: 0.9876 - val_loss: 0.0381 - val_acc: 0.9879\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 36s 594us/step - loss: 0.0331 - acc: 0.9898 - val_loss: 0.0393 - val_acc: 0.9880\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 36s 592us/step - loss: 0.0273 - acc: 0.9914 - val_loss: 0.0350 - val_acc: 0.9901\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 35s 585us/step - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0333 - val_acc: 0.9901\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 36s 602us/step - loss: 0.0220 - acc: 0.9932 - val_loss: 0.0250 - val_acc: 0.9915\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 37s 609us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0307 - val_acc: 0.9903\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 36s 600us/step - loss: 0.0174 - acc: 0.9943 - val_loss: 0.0217 - val_acc: 0.9938\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 35s 591us/step - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0279 - val_acc: 0.9914\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 38s 636us/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0270 - val_acc: 0.9924\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 36s 601us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0258 - val_acc: 0.9933\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 36s 595us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0296 - val_acc: 0.9929\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 36s 594us/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0269 - val_acc: 0.9930\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 36s 598us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0298 - val_acc: 0.9928\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 36s 598us/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0320 - val_acc: 0.9921\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 36s 601us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0294 - val_acc: 0.9921\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 36s 597us/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0268 - val_acc: 0.9939\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 36s 600us/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0251 - val_acc: 0.9936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJv5ln6lx6CB",
        "colab_type": "code",
        "outputId": "bc239f37-5724-4e79-bbd5-4b1360b9b322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,acc,'b',label='Train_acc')\n",
        "plt.plot(epochs,val_acc,'g',label='Val_acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52eb7aqKx9m0",
        "colab_type": "code",
        "outputId": "616a0ae1-9414-4a9b-b2d4-5306b363bb0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 122us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.025061630396601777, 0.9936]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kTtqIqS7por",
        "colab_type": "text"
      },
      "source": [
        "achieved val_acc 99.39...\n",
        "but,there is gap between the val and train curves..\n",
        "reason may be overfitting...\n",
        "this can be tackled with introducing l1 regressor.."
      ]
    }
  ]
}